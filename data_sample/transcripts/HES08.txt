 Sur l'écran, en haut à gauche, vous pouvez voir un tigre.
 En dessous, un tigre.
 En dessous, un tigre.
 En dessous, un tigre.
 Normalement, je devrais continuer comme ça quelques milliers de fois,
 mais je n'ai que 180 secondes, donc je vais abréger un peu.
 En haut à droite, un ours.
 En dessous, un ours. En dessous, un ours, etc.
 Au milieu, en haut, il y a une nouvelle image.
 Qui parmi vous pense qu'il s'agit d'un ours ?
 Très bien. Qui pour un tigre ?
 Parfait. J'ai une bonne nouvelle pour la majorité d'entre vous,
 à part le monsieur ici.
 Vous n'êtes pas encore complètement dépassés par l'IA, l'intelligence artificielle.
 Ce que vous venez de faire s'appelle dans le domaine informatique
 du machine learning, ou apprentissage automatique.
 Vous avez appris à effectuer une tâche, ici distinguer un ours d'un tigre,
 à partir d'exemples.
 Mon travail consiste à faire faire la même chose à des ordinateurs.
 Les applications de l'apprentissage automatique sont innombrables.
 Analyse d'images médicales, conception de véhicules autonomes,
 mais aussi reconnaissance vocale, traduction automatique,
 on pourrait passer la soirée à les lister.
 Pourtant, ce type d'approche a un problème.
 A l'heure actuelle, pour apprendre efficacement, un ordinateur a besoin
 de milliers d'exemples, qui sont longs et souvent coûteux à collecter.
 Pourrait-on faire mieux ? A priori, oui.
 Même si vous n'aviez jamais vu ni ours ni tigre,
 vous pourriez sûrement apprendre à partir de ces quelques exemples.
 Pourrait-on faire encore mieux ?
 Peut-être qu'avec des exemples bien choisis, une seule image par catégorie suffirait.
 Pourrait-on faire encore, encore mieux ?
 Peut-on reconnaître quelque chose que l'on n'a jamais vu ?
 Par exemple, pourriez-vous reconnaître le greubleutis parmi les trois images du bas ?
 Dans l'immédiat, non.
 Mais si je vous donne l'information qu'un greubleutis, c'est comme un tigre,
 mais c'est bleu, ça devient tout de suite beaucoup plus facile.
 L'objectif de mes recherches est d'intégrer cette capacité à une intelligence artificielle.
 Plus spécifiquement, je modélise les relations existant entre des images
 et des descriptions textuelles, afin que cette IA puisse reconnaître des objets
 qu'elle n'a encore jamais vus.
 C'est ce qu'on appelle du « zero-shot learning », de l'apprentissage avec zéro exemple.
 Comment ça marche ? C'est simple, j'applique aux images une série de produits tensoriels
 entrecoupés de non-linearités et je maximise une fonction de compatibilité bilinaire
 entre la maîtrise résultante et les vecteurs sémantiques.
 En gros, j'utilise pas mal de maths.
 Mais est-ce que ça marche ?
 Pour le savoir, j'ai d'abord entraîné mon modèle à identifier l'espèce d'un oiseau
 en utilisant des serveurs d'images et de textes.
 Je lui ai ensuite demandé de catégoriser des photos d'oiseaux appartenant à 50 nouvelles espèces
 dont il n'a jamais vu un seul spécimen et sur lesquelles les seules informations disponibles
 étaient des descriptions textuelles.
 Mon modèle a pu identifier correctement l'espèce dans la majorité des cas.
 A terme, j'espère que ces travaux contribueront à simplifier l'utilisation de l'IA
 lorsque pas ou peu de données sont disponibles,
 pour par exemple diagnostiquer des maladies rares,
 identifier de nouvelles protéines
 ou bien sûr reconnaître des greubles étis.
 Je vous remercie.
